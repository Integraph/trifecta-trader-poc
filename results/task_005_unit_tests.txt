ERROR conda.cli.main_run:execute(127): `conda run python -m pytest tests/ -v --ignore=tests/test_reasoning_comparison.py` failed. (See above for error)
============================= test session starts ==============================
platform darwin -- Python 3.13.12, pytest-9.0.2, pluggy-1.6.0 -- /opt/homebrew/Caskroom/miniconda/base/envs/tradingagents/bin/python
cachedir: .pytest_cache
rootdir: /Users/jeffbezenyan/Projects/Trifecta_Trader/App/trifecta-trader-poc
configfile: pyproject.toml
plugins: anyio-4.12.1, langsmith-0.7.9
collecting ... collected 54 items

tests/test_compare_runs.py::TestCompareRuns::test_load_result PASSED     [  1%]
tests/test_compare_runs.py::TestCompareRuns::test_result_to_score PASSED [  3%]
tests/test_compare_runs.py::TestCompareRuns::test_result_to_score_missing_quality PASSED [  5%]
tests/test_config.py::test_tradingagents_importable PASSED               [  7%]
tests/test_config.py::test_config_creation PASSED                        [  9%]
tests/test_config.py::test_data_vendors_default_to_yfinance PASSED       [ 11%]
tests/test_config.py::test_results_directory PASSED                      [ 12%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_default_config PASSED [ 14%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_predefined_configs_exist PASSED [ 16%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_all_cloud_config PASSED [ 18%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_hybrid_qwen_config PASSED [ 20%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_hybrid_mistral_config PASSED [ 22%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_aggressive_configs_use_local_for_deep PASSED [ 24%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_hybrid_qwen32_config PASSED [ 25%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_hybrid_aggressive_qwen32_config PASSED [ 27%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_to_dict PASSED       [ 29%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_custom_config PASSED [ 31%]
tests/test_hybrid_llm.py::TestHybridLLMConfig::test_to_dict_format PASSED [ 33%]
tests/test_hybrid_llm.py::TestHybridGraphSetup::test_hybrid_graph_setup_importable PASSED [ 35%]
tests/test_hybrid_llm.py::TestHybridGraphSetup::test_hybrid_graph_setup_accepts_three_llms PASSED [ 37%]
tests/test_local_tool_calling.py::test_tool_calling_basic[qwen2.5:14b] PASSED [ 38%]
tests/test_local_tool_calling.py::test_tool_calling_basic[mistral-small:22b] FAILED [ 40%]
tests/test_local_tool_calling.py::test_tool_calling_basic[qwen2.5:32b] PASSED [ 42%]
tests/test_local_tool_calling.py::test_tool_calling_multi_tool[qwen2.5:14b] PASSED [ 44%]
tests/test_local_tool_calling.py::test_tool_calling_multi_tool[mistral-small:22b] FAILED [ 46%]
tests/test_local_tool_calling.py::test_tool_calling_multi_tool[qwen2.5:32b] PASSED [ 48%]
tests/test_local_tool_calling.py::test_reasoning_quality[qwen2.5:14b] PASSED [ 50%]
tests/test_local_tool_calling.py::test_reasoning_quality[mistral-small:22b] PASSED [ 51%]
tests/test_local_tool_calling.py::test_reasoning_quality[qwen2.5:32b] PASSED [ 53%]
tests/test_pipeline.py::test_decision_not_repeated PASSED                [ 55%]
tests/test_pipeline.py::test_conditional_logic_defaults PASSED           [ 57%]
tests/test_pipeline.py::test_conditional_logic_risk_round_limit PASSED   [ 59%]
tests/test_pipeline.py::test_conditional_logic_config_not_passed PASSED  [ 61%]
tests/test_pipeline.py::test_propagator_defaults PASSED                  [ 62%]
tests/test_quality_scorer.py::TestQualityScorer::test_high_quality_output PASSED [ 64%]
tests/test_quality_scorer.py::TestQualityScorer::test_low_quality_output PASSED [ 66%]
tests/test_quality_scorer.py::TestQualityScorer::test_inconsistent_decision PASSED [ 68%]
tests/test_quality_scorer.py::TestQualityScorer::test_comparison_report PASSED [ 70%]
tests/test_signal_processing.py::TestExtractDecision::test_final_transaction_proposal_hold PASSED [ 72%]
tests/test_signal_processing.py::TestExtractDecision::test_final_transaction_proposal_buy PASSED [ 74%]
tests/test_signal_processing.py::TestExtractDecision::test_final_transaction_proposal_sell PASSED [ 75%]
tests/test_signal_processing.py::TestExtractDecision::test_ignores_negation_not_recommending_sell PASSED [ 77%]
tests/test_signal_processing.py::TestExtractDecision::test_multiple_proposals_takes_last PASSED [ 79%]
tests/test_signal_processing.py::TestExtractDecision::test_recommendation_pattern PASSED [ 81%]
tests/test_signal_processing.py::TestExtractDecision::test_no_markdown_bold PASSED [ 83%]
tests/test_signal_processing.py::TestExtractDecision::test_empty_input PASSED [ 85%]
tests/test_signal_processing.py::TestExtractDecision::test_no_decision_found PASSED [ 87%]
tests/test_signal_processing.py::TestExtractDecision::test_sell_in_reasoning_hold_in_proposal PASSED [ 88%]
tests/test_signal_processing.py::TestExtractDecision::test_case_insensitive PASSED [ 90%]
tests/test_signal_processing.py::TestExtractDecision::test_standalone_decision_fallback PASSED [ 92%]
tests/test_signal_processing.py::TestEdgeCases::test_buy_with_extra_text PASSED [ 94%]
tests/test_signal_processing.py::TestEdgeCases::test_hold_with_conditions PASSED [ 96%]
tests/test_signal_processing.py::TestEdgeCases::test_hash_prefix_on_proposal PASSED [ 98%]
tests/test_signal_processing.py::TestEdgeCases::test_conviction_level_does_not_interfere PASSED [100%]

=================================== FAILURES ===================================
__________________ test_tool_calling_basic[mistral-small:22b] __________________

model_name = 'mistral-small:22b'

    @pytest.mark.parametrize("model_name", OLLAMA_MODELS)
    def test_tool_calling_basic(model_name):
        """Test if the model can produce a valid tool call."""
        llm = ChatOpenAI(
            model=model_name,
            base_url="http://localhost:11434/v1",
            api_key="ollama",
            temperature=0,
        )
    
        tool = _get_test_tool()
        llm_with_tools = llm.bind_tools([tool])
    
        result = llm_with_tools.invoke(
            "What is the stock price of AAPL on 2026-02-27?"
        )
    
        has_tool_calls = hasattr(result, "tool_calls") and len(result.tool_calls) > 0
    
        if has_tool_calls:
            tc = result.tool_calls[0]
            assert tc["name"] == "get_stock_price", f"Wrong tool name: {tc['name']}"
            assert "ticker" in tc["args"], f"Missing 'ticker' arg: {tc['args']}"
            print(f"\n  {model_name}: Tool calling WORKS")
            print(f"   Tool call: {tc['name']}({tc['args']})")
        else:
            print(f"\n  {model_name}: Tool calling FAILED")
            print(f"   Response (first 200 chars): {result.content[:200]}")
>           pytest.fail(
                f"{model_name} did not produce tool calls. "
                f"Response: {result.content[:200]}"
            )
E           Failed: mistral-small:22b did not produce tool calls. Response:  [{"name":"get_stock_price","arguments":{"ticker":"AAPL","date":"2026-02-27"}}]

tests/test_local_tool_calling.py:86: Failed
----------------------------- Captured stdout call -----------------------------

  mistral-small:22b: Tool calling FAILED
   Response (first 200 chars):  [{"name":"get_stock_price","arguments":{"ticker":"AAPL","date":"2026-02-27"}}]
_______________ test_tool_calling_multi_tool[mistral-small:22b] ________________

model_name = 'mistral-small:22b'

    @pytest.mark.parametrize("model_name", OLLAMA_MODELS)
    def test_tool_calling_multi_tool(model_name):
        """Test if the model can choose the right tool when multiple are available."""
        from langchain_core.tools import tool
    
        @tool
        def get_stock_price(ticker: str) -> str:
            """Get the current stock price for a ticker symbol."""
            return json.dumps({"ticker": ticker, "price": 185.50})
    
        @tool
        def get_company_news(company: str) -> str:
            """Get recent news articles about a company."""
            return json.dumps({"company": company, "articles": []})
    
        llm = ChatOpenAI(
            model=model_name,
            base_url="http://localhost:11434/v1",
            api_key="ollama",
            temperature=0,
        )
    
        llm_with_tools = llm.bind_tools([get_stock_price, get_company_news])
    
        result = llm_with_tools.invoke("What is AAPL trading at?")
    
        has_tool_calls = hasattr(result, "tool_calls") and len(result.tool_calls) > 0
        if has_tool_calls:
            tc = result.tool_calls[0]
            assert tc["name"] == "get_stock_price", (
                f"Model called wrong tool: {tc['name']} (expected get_stock_price)"
            )
            print(f"\n  {model_name}: Multi-tool selection WORKS -- chose {tc['name']}")
        else:
            print(f"\n  {model_name}: Multi-tool selection FAILED -- no tool calls")
>           pytest.fail(f"{model_name} did not produce tool calls")
E           Failed: mistral-small:22b did not produce tool calls

tests/test_local_tool_calling.py:127: Failed
----------------------------- Captured stdout call -----------------------------

  mistral-small:22b: Multi-tool selection FAILED -- no tool calls
=========================== short test summary info ============================
FAILED tests/test_local_tool_calling.py::test_tool_calling_basic[mistral-small:22b] - Failed: mistral-small:22b did not produce tool calls. Response:  [{"name":"get_stock_price","arguments":{"ticker":"AAPL","date":"2026-02-27"}}]
FAILED tests/test_local_tool_calling.py::test_tool_calling_multi_tool[mistral-small:22b] - Failed: mistral-small:22b did not produce tool calls
=================== 2 failed, 52 passed in 87.68s (0:01:27) ====================

